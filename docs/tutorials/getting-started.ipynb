{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b647ca",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "To demonstrate how BayKer can be used to model kernel phases, we will use the PHARO data from the [Xara tutorial](https://frantzmartinache.eu/xara_doc/04_pharo_data_analysis_example.html).\n",
    "\n",
    "## Data Download\n",
    "\n",
    "We will first download the data from Xara's GitHub if it is not already available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93beb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "tgt_filename = \"tgt_cube.fits\"\n",
    "cal_filename = \"ca2_cube.fits\"\n",
    "pupil_filename = \"p3k_med_grey_model.fits\"\n",
    "dir_url = \"https://github.com/fmartinache/xara/raw/master/tests/data/PHARO/\"\n",
    "data_dir = Path(\"./data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for filename in [tgt_filename, cal_filename, pupil_filename]:\n",
    "    download_path = data_dir /filename\n",
    "    url = dir_url + filename\n",
    "    if not download_path.exists():\n",
    "        urllib.request.urlretrieve(url, download_path)\n",
    "        print(f\"Downloaded {download_path}\")\n",
    "    else:\n",
    "        print(f\"{download_path} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc6100",
   "metadata": {},
   "source": [
    "## Kernel Phase Extraction\n",
    "\n",
    "BayKer works with _extracted_ kernel phase data. This means we need to process the image cubes with Xara before doing our analysis.\n",
    "We will not cover this step in detail as it is already explained in the [Xara documentation](https://frantzmartinache.eu/xara_doc/04_pharo_data_analysis_example.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xara.kpo import KPO\n",
    "from astropy.io import fits\n",
    "\n",
    "model_path = data_dir / pupil_filename\n",
    "kpo_tgt = KPO(fname=model_path)\n",
    "kpo_cal = kpo_tgt.copy()\n",
    "\n",
    "pscale = 25.0  # plate scale of the image in mas/pixels\n",
    "wl = 2.145e-6  # central wavelength in meters (Hayward paper)\n",
    "\n",
    "tgt_path = data_dir / tgt_filename\n",
    "with fits.open(tgt_path) as hdul:\n",
    "    tgt_cube = hdul[0].data\n",
    "cal_path = data_dir / cal_filename\n",
    "with fits.open(cal_path) as hdul:\n",
    "    cal_cube = hdul[0].data\n",
    "\n",
    "kpo_tgt.extract_KPD_single_cube(tgt_cube, pscale, wl,target=\"alpha Ophiuchi\", recenter=True)\n",
    "kpo_cal.extract_KPD_single_cube(cal_cube, pscale, wl, target=\"epsilon Herculis\", recenter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3c9bb",
   "metadata": {},
   "source": [
    "We now have raw kernel phases for our science target and our reference star.\n",
    "Kernel phase data is stored as `KPO` objects (KPO stands for Kernel Phase Observation).\n",
    "Their most important attributes are:\n",
    "\n",
    "- `KPDT`: A list of extracted kernel phase datasets each with shape `(nint, nkp)`.\n",
    "- `KPSIG`: A list of associated errors with the same shape as `KPDT`, if the errors were calculated\n",
    "- `CWAVEL`: The central wavelength of the data\n",
    "- `kpi`: The `KPI` object associated with the dataset. KPI stands for Kernel Phase Information and stores information about the pupil model used to convert the images to kernel phases. Its important attributes are:\n",
    "  - `VAC`: The pupil-plane coordinate of the pupil models\n",
    "  - `UVC`: The UV-plane coordinate of the baselines in the pupil model\n",
    "  - `KPM`: The kernel phase matrix used to convert Fourier phases to kernel phases, with shape `(nkp, nph)`\n",
    "\n",
    "In our case, `KPSIG` was not estimated, and `KPDT` has one element with shape `(100, 1048)`.\n",
    "\n",
    "## Kernel Phase Calibration\n",
    "\n",
    "Before taking our analysis further, we will estimate the kernel phase variance across the 100 frames and use the median as our best estimate. Since the KPO data format is a bit convoluted (with lists of arrays), we have helper function to do this.\n",
    "\n",
    "We can then calibrate the kernel phases by subtracting the reference from the science target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23bef7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import bayker.utils as ut\n",
    "kpo_tgt = ut.average_kpo(kpo_tgt)\n",
    "kpo_cal = ut.average_kpo(kpo_cal)\n",
    "kpo = ut.calibrate_kpo(kpo_tgt, kpo_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d05417",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(kpo.KPDT[0].shape[-1])\n",
    "\n",
    "plt.errorbar(x, kpo_tgt.KPDT[0][0], kpo_tgt.KPSIG[0][0], fmt=\"k.\", label=\"Science\")\n",
    "plt.errorbar(x, kpo_cal.KPDT[0][0], kpo_cal.KPSIG[0][0], fmt=\"b.\", label=\"Reference\")\n",
    "plt.xlabel(\"Kernel Index\")\n",
    "plt.ylabel(\"Kernel Phase [rad]\")\n",
    "plt.title(\"Raw Kernel Phases\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, kpo.KPDT[0][0], kpo.KPSIG[0][0], fmt=\"k.\")\n",
    "plt.xlabel(\"Kernel Index\")\n",
    "plt.ylabel(\"Kernel Phase [rad]\")\n",
    "plt.title(\"Calibrated Kernel Phases\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1622001",
   "metadata": {},
   "source": [
    "## Colinearity Map\n",
    "\n",
    "As you might have noticed above, kernel phase data is not exactly intuitive to look at...\n",
    "Before diving into the Bayesian analysis, we can get a quick sense of whether there is some signal in the data by doing a _colinearity map_ (also called _cross-correlation_ map).\n",
    "Basically, this map shows a dot product of the data and a binary signal for all positions on a grid.\n",
    "The contrast remains fixed but would only scale the entire map by a constant factor anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayker.search import get_colinearity_map\n",
    "cmap, pos_grid = get_colinearity_map(kpo, 100, (-400, 400), return_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_idx, ra_idx = np.unravel_index(np.argmax(cmap), (100, 100))\n",
    "max_ra = pos_grid[\"ra\"][ra_idx]\n",
    "max_dec = pos_grid[\"dec\"][dec_idx]\n",
    "plt.imshow(cmap, extent=(-400, 400, -400, 400))\n",
    "plt.plot(max_ra, max_dec, \"rx\")\n",
    "plt.xlabel(\"RA [mas]\")\n",
    "plt.ylabel(\"Dec [mas]\")\n",
    "plt.title(f\"Colinearity Map with Max (RA, Dec)=({max_ra:.2f}, {max_dec:.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d061d1",
   "metadata": {},
   "source": [
    "## Bayesian Model\n",
    "\n",
    "We are now ready to build our Bayesian kernel phase model.\n",
    "BayKer provides `KernelModel` class.\n",
    "We need to give it a parameter dictionary with the priors and a `KPO` object with the data and pupil model information.\n",
    "We can also optionally specify the binary model parametrization with the `pos_param` argument.\n",
    "Here we stick to the default `seppa`. The alternative would be `radec`.\n",
    "\n",
    "`KernelModel` is built with `simpple` so it has all the usual attributes and methods (`parameters`, `forward`, `log_likelihood`, etc.).\n",
    "The forward model in `KernelModel` takes parameter values as input and returns the associated kernel phases.\n",
    "There is also an optional `sigma` parameter that will be added in quadrature to the kernel phase errors in the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpple.distributions as sdist\n",
    "from bayker.model import KernelModel\n",
    "\n",
    "parameters = {\n",
    "    \"sigma\": sdist.LogUniform(1e-5, 10.0),\n",
    "    \"sep\": sdist.LogUniform(1e-5, 400.0),\n",
    "    \"pa\": sdist.Uniform(0, 360.0),\n",
    "    \"cr\": sdist.LogUniform(1e-5, 1.0),\n",
    "}\n",
    "model = KernelModel(parameters, kpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d721057",
   "metadata": {},
   "source": [
    "### Prior samples\n",
    "\n",
    "First, let us look at prior samples in a corner plot to make sure the model specification is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "prior_samples = model.get_prior_samples(10000)\n",
    "fig = corner.corner(prior_samples)\n",
    "fig.suptitle(\"Prior Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a6730",
   "metadata": {},
   "source": [
    "And let us also look at prior predictive samples to make sure forward models drawn from the prior cover the data and look reasonable.\n",
    "<!-- TODO: Link to API docs -->\n",
    "To plot the data, we will use the helper function `plot_data` from `bayker.plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayker.plot import plot_data\n",
    "\n",
    "plot_data(kpo)\n",
    "prior_pred_samples = model.get_prior_pred(100)\n",
    "for i in range(prior_pred_samples.shape[0]):\n",
    "    plt.plot(\n",
    "        model.kpo.x,\n",
    "        prior_pred_samples[i],\n",
    "        alpha=0.1,\n",
    "        color=\"C0\",\n",
    "        label=\"Prior samples\" if i == 0 else None,\n",
    "        zorder=1000,\n",
    "    )\n",
    "plt.title(\"Prior Predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96925f",
   "metadata": {},
   "source": [
    "### Sampling the Posterior\n",
    "\n",
    "Now we can sample the posterior. We will use [UltraNest](https://johannesbuchner.github.io/UltraNest/index.html) for this example.\n",
    "Ultranest shows a lot of debug information so we wrap it in a function that turns off logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from ultranest import ReactiveNestedSampler\n",
    "from ultranest.stepsampler import SliceSampler, generate_mixture_random_direction\n",
    "def run_ultranest(model: KernelModel) -> ReactiveNestedSampler:\n",
    "    wrapped = [k.startswith(\"pa\") for k in model.keys()]\n",
    "    nested_sampler = ReactiveNestedSampler(\n",
    "        model.keys(),\n",
    "        model.log_likelihood,\n",
    "        model.prior_transform,\n",
    "        wrapped_params=wrapped,\n",
    "    )\n",
    "    if len(model.keys()) > 4:\n",
    "        nsteps = len(model.keys()) * 2\n",
    "        nested_sampler.stepsampler = SliceSampler(\n",
    "            nsteps=nsteps,\n",
    "            generate_direction=generate_mixture_random_direction,\n",
    "        )\n",
    "    # HACK: Force all existing loggers to WARNING\n",
    "    # Ultranest has DEBUG when running after corner for some reason\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "    for logger_name in logging.root.manager.loggerDict:\n",
    "        logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "    nested_sampler.run(show_status=False);\n",
    "    return nested_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_sampler = run_ultranest(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_sampler.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f3e57",
   "metadata": {},
   "source": [
    "This looks good! Let's see graphically how the model matches the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = model.get_posterior_pred(nested_sampler.results[\"samples\"].T, 100)\n",
    "# TODO: Add residuals\n",
    "# TODO: Inflate with sigma term\n",
    "plot_data(model.kpo)\n",
    "for i in range(posterior_samples.shape[0]):\n",
    "    plt.plot(\n",
    "        model.kpo.x,\n",
    "        posterior_samples[i],\n",
    "        alpha=0.1,\n",
    "        color=\"C0\",\n",
    "        label=\"Posterior samples\" if i == 0 else None,\n",
    "        zorder=1000,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b05f4",
   "metadata": {},
   "source": [
    "Since the x-axis is just a numerical index, it is also quite common to just show the model vs the data.\n",
    "We will use the median samples from the sampler for this.\n",
    "We also show the effect of the `sigma` term in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_p = np.median(nested_sampler.results[\"samples\"], axis=0)\n",
    "med_p_dict = dict(zip(model.keys(), med_p))\n",
    "med_pred = model.forward(med_p)\n",
    "plt.errorbar(med_pred, model.kpo.kp, yerr=model.kpo.ekp, fmt=\"k.\", label=\"Data\", capsize=1)\n",
    "inflated_ekp = np.sqrt(model.kpo.ekp**2 + med_p_dict[\"sigma\"]**2)\n",
    "plt.errorbar(med_pred, model.kpo.kp, yerr=inflated_ekp, ecolor=\"r\", fmt=\"none\", label=\"Data\", alpha=0.5, capsize=2)\n",
    "plt.axline((0, 0), slope=1, label=\"1:1 Line\")\n",
    "plt.legend()\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2834c079",
   "metadata": {},
   "source": [
    "## Bayesian Model Comparison\n",
    "\n",
    "The nested sampling run above gave us the Bayesian evidence for a binary model.\n",
    "The null model corresponding to a centered point source for a kernel phase dataset is a straight line.\n",
    "However, to provide a fair comparison to our binary model above, we must allow a \"sigma\" term to inflate the error bars around the straight line.\n",
    "\n",
    "We can build such a model with BayKer using the `model_type` argument and sample it with UltraNest again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e756354",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_single = {\"sigma\": sdist.LogUniform(1e-5, 10.0)}\n",
    "model_single = KernelModel(parameters_single, kpo, model_type=\"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3230c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_sampler_single = run_ultranest(model_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_sampler_single.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc900af",
   "metadata": {},
   "source": [
    "That was much quicker than the binary model!\n",
    "Let us now extract the evidence for both models and calculate the bayes factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"lnZ single: {nested_sampler_single.results['logz']:.2f} +/- {nested_sampler_single.results['logzerr']:.2f}\")\n",
    "print(\n",
    "    f\"lnZ binary: {nested_sampler.results['logz']:.2f} +/- {nested_sampler.results['logzerr']:.2f}\"\n",
    ")\n",
    "lnK = nested_sampler.results[\"logz\"] - nested_sampler_single.results[\"logz\"]\n",
    "lnK_err = np.sqrt(\n",
    "    nested_sampler_single.results[\"logzerr\"] ** 2 + nested_sampler.results[\"logzerr\"] ** 2\n",
    ")\n",
    "print(f\"lnK binary - single: {lnK:.2f} +/- {lnK_err:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220dcb4e",
   "metadata": {},
   "source": [
    "Without too much surprise, the Bayes factor very strongly favors the binary model compared to a straight line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
